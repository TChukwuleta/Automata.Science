{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTING NECESSARY PACKAGES\n",
    "import tweepy\n",
    "import csv\n",
    "import pandas as pd\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy.streaming import StreamListener\n",
    "import json\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "import string\n",
    "#import preprocessor as p\n",
    "import os\n",
    "import time\n",
    "import jsonpickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATING ACCESS TO TWITTER API\n",
    "API_key = 'o6ChEIbTixjt3uIKX5nGHaFVt'\n",
    "API_secret = 'kSXbjj6XgZJ6RqdbQuk6a8S5uBGwDsSOMT5fmI3OFAk7f046va'\n",
    "access_token = '1219619295992467456-QKujRwIh7mLKT9doPkc2KfgNojHXNm'\n",
    "access_token_secret = 'Ze6pb9TRfJ3KKiYVPyaZpTodjDF7oQokXRwQhdYS5zGph'\n",
    "auth = tweepy.OAuthHandler(API_key, API_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit = True, wait_on_rate_limit_notify = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Seyi Makinde', '#TheLockdown', '#TachaTheVoice', '#RCCGLOVEINACTION', '#TradeOnPrestmit', '#Coronavirustruth', 'Teni', 'Congratulations Moyin', 'Sarz', '300k in Ibadan', 'dremo', 'Mikel', 'Rally', 'Pheelz', 'Sydney Talker', 'Masterkraft', 'Reeky', 'Quick Recovery', 'Ooni', 'survivor', 'Black Widow', 'Infectious Disease Centre', 'Adonis', 'Dami Duro', 'Data Analysis', 'Don Jazzy', 'Ragnar', 'Giroud', 'President Muhammadu Buhari', 'Jon Snow', '#Tension', '#BibleStudyWithKumuyi', '#GraduationChallenge', '#BlessingsByMudi', '#NationalDoctorsDay', '#LearnConnect', '#NaijaAtHome', '#nomoredreamiscoming', '#BudgetingwithV', '#allthingsareready', '#foshstudiogiveaway', '#DontRushChallange', '#Camon15Launch', '#CutDataPrices', '#WhenCoronaVirusIsOver', '#BuhariAddressNigerians', '#StayAtHomeAndStaySafe', '#ultimatelovefinale', '#mondaythoughts', '#centremmyayevideo']\n"
     ]
    }
   ],
   "source": [
    "#EXTRACTING TOP TRENDS\n",
    "NIGERIA_WOE_ID = 23424908\n",
    "nig_trends = api.trends_place(NIGERIA_WOE_ID)\n",
    "#trends = json.loads(json.dumps(nig_trends, indent=1))\n",
    "#for trend in trends[0][\"trends\"]:\n",
    "#    print((trend[\"name\"]).strip(\"#\"))\n",
    "#OR\n",
    "\n",
    "data = nig_trends[0]\n",
    "#grab the trends\n",
    "trendss = data['trends']\n",
    "#Grab the name from each trend\n",
    "names = [trend['name'] for trend in trendss]\n",
    "names = list(names)\n",
    "#Put all the names together with a ' ' seperating them\n",
    "#trendsName = ' '.join(names)\n",
    "print(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_per_query = 100\n",
    "max_tweets = 30\n",
    "fName = 'AS1_30_03_20.csv' #Where i save the tweets\n",
    "fName2 = 'AS2_30_03_20.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading the tweeets..takes some time..\n"
     ]
    }
   ],
   "source": [
    "since_id = None\n",
    "max_id = -1\n",
    "tweet_count = 0\n",
    "print(\"Downloading the tweeets..takes some time..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading hashtag#TheLockdown\n",
      "Successfully downloaded 100 tweets\n",
      "Successfully downloaded 200 tweets\n",
      "Successfully downloaded 300 tweets\n",
      "Successfully downloaded 400 tweets\n",
      "Successfully downloaded 500 tweets\n",
      "Successfully downloaded 600 tweets\n",
      "Successfully downloaded 700 tweets\n",
      "Successfully downloaded 800 tweets\n",
      "Successfully downloaded 900 tweets\n",
      "Successfully downloaded 1000 tweets\n",
      "Successfully downloaded 1100 tweets\n",
      "Successfully downloaded 1200 tweets\n",
      "Successfully downloaded 1300 tweets\n",
      "Successfully downloaded 1400 tweets\n",
      "Successfully downloaded 1500 tweets\n",
      "Successfully downloaded 1600 tweets\n",
      "Successfully downloaded 1700 tweets\n",
      "Successfully downloaded 1800 tweets\n",
      "Successfully downloaded 1900 tweets\n",
      "Successfully downloaded 2000 tweets\n",
      "Successfully downloaded 2100 tweets\n",
      "Successfully downloaded 2200 tweets\n",
      "Successfully downloaded 2300 tweets\n",
      "Successfully downloaded 2400 tweets\n",
      "Successfully downloaded 2500 tweets\n",
      "Successfully downloaded 2600 tweets\n",
      "Successfully downloaded 2700 tweets\n",
      "Successfully downloaded 2800 tweets\n",
      "Successfully downloaded 2900 tweets\n",
      "Successfully downloaded 3000 tweets\n",
      "Successfully downloaded 3100 tweets\n",
      "Successfully downloaded 3200 tweets\n",
      "Successfully downloaded 3300 tweets\n",
      "Successfully downloaded 3400 tweets\n",
      "Successfully downloaded 3500 tweets\n",
      "Successfully downloaded 3600 tweets\n",
      "Successfully downloaded 3700 tweets\n",
      "Successfully downloaded 3800 tweets\n",
      "Successfully downloaded 3900 tweets\n",
      "Successfully downloaded 4000 tweets\n",
      "Successfully downloaded 4100 tweets\n",
      "Successfully downloaded 4200 tweets\n",
      "Successfully downloaded 4300 tweets\n",
      "Successfully downloaded 4400 tweets\n",
      "Successfully downloaded 4500 tweets\n",
      "Successfully downloaded 4600 tweets\n",
      "Successfully downloaded 4700 tweets\n",
      "Successfully downloaded 4800 tweets\n",
      "Successfully downloaded 4900 tweets\n",
      "Successfully downloaded 5000 tweets\n",
      "Successfully downloaded 5100 tweets\n",
      "Successfully downloaded 5200 tweets\n",
      "Successfully downloaded 5300 tweets\n",
      "Successfully downloaded 5400 tweets\n",
      "Successfully downloaded 5500 tweets\n",
      "Successfully downloaded 5600 tweets\n",
      "Successfully downloaded 5700 tweets\n",
      "Successfully downloaded 5800 tweets\n",
      "Successfully downloaded 5900 tweets\n",
      "Successfully downloaded 6000 tweets\n",
      "Successfully downloaded 6100 tweets\n",
      "Successfully downloaded 6200 tweets\n",
      "Successfully downloaded 6300 tweets\n",
      "Successfully downloaded 6400 tweets\n",
      "Successfully downloaded 6500 tweets\n",
      "Successfully downloaded 6600 tweets\n",
      "Successfully downloaded 6700 tweets\n",
      "Successfully downloaded 6800 tweets\n",
      "Successfully downloaded 6900 tweets\n",
      "Successfully downloaded 7000 tweets\n",
      "Successfully downloaded 7100 tweets\n",
      "Successfully downloaded 7200 tweets\n",
      "Successfully downloaded 7300 tweets\n",
      "Successfully downloaded 7400 tweets\n",
      "Successfully downloaded 7500 tweets\n",
      "Successfully downloaded 7600 tweets\n",
      "Successfully downloaded 7700 tweets\n",
      "Successfully downloaded 7800 tweets\n",
      "Successfully downloaded 7900 tweets\n",
      "Successfully downloaded 8000 tweets\n",
      "Successfully downloaded 8100 tweets\n",
      "Successfully downloaded 8200 tweets\n",
      "Successfully downloaded 8300 tweets\n",
      "Successfully downloaded 8400 tweets\n",
      "Successfully downloaded 8500 tweets\n",
      "Successfully downloaded 8600 tweets\n",
      "Successfully downloaded 8700 tweets\n",
      "Successfully downloaded 8800 tweets\n",
      "Successfully downloaded 8900 tweets\n",
      "Successfully downloaded 9000 tweets\n",
      "Successfully downloaded 9100 tweets\n",
      "Successfully downloaded 9200 tweets\n",
      "Successfully downloaded 9300 tweets\n",
      "Successfully downloaded 9400 tweets\n",
      "Successfully downloaded 9500 tweets\n",
      "Successfully downloaded 9600 tweets\n",
      "Successfully downloaded 9700 tweets\n",
      "Successfully downloaded 9800 tweets\n",
      "Successfully downloaded 9900 tweets\n",
      "Successfully downloaded 10000 tweets\n",
      "A total of 10000 tweets are downloaded and saved to AS1_30_03_20.csv\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'st' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-2b6a0ceaa35e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"A total of {0} tweets are downloaded and saved to {1}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfName\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Total time taken is \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mst\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"seconds.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'st' is not defined"
     ]
    }
   ],
   "source": [
    "search_query='#TheLockdown'\n",
    "x=0\n",
    "with open(fName,'w') as f:\n",
    "    print(\"Downloading hashtag\" + search_query)\n",
    "    while(tweet_count<max_tweets):\n",
    "        try:\n",
    "            if(max_id<=0):\n",
    "                if(not since_id):\n",
    "                    new_tweets = api.search(q=search_query,count=tweets_per_query,lang=\"en\",tweet_mode='extended')\n",
    "\n",
    "                else:\n",
    "                    new_tweets = api.search(q=search_query,count=tweets_per_query,lang=\"en\",tweet_mode='extended',since_id=since_id)\n",
    "            else:\n",
    "                if(not since_id):\n",
    "                    new_tweets = api.search(q=search_query,count=tweets_per_query,lang=\"en\",tweet_mode='extended',max_id=str(max_id-1))\n",
    "                else:\n",
    "                    new_tweets = api.search(q=search_query,count=tweets_per_query,lang=\"en\",tweet_mode='extended',max_id=str(max_id-1),since_id=since_id)\n",
    "\n",
    "            # Tweets Exhausted\n",
    "            if(not new_tweets):\n",
    "                print(\"No more tweets found!!\")\n",
    "                break\n",
    "            # write all the new_tweets to a json file\n",
    "            for tweet in new_tweets:\n",
    "                f.write(jsonpickle.encode(tweet._json,unpicklable=False)+'\\n')\n",
    "                tweet_count+=len(new_tweets)\n",
    "                print(\"Successfully downloaded {0} tweets\".format(tweet_count))\n",
    "                max_id=new_tweets[-1].id\n",
    "        # in case of any error\n",
    "        except tweepy.TweepError as e:\n",
    "                print(\"Some error!!:\"+str(e))\n",
    "                break\n",
    "end = time.time()\n",
    "\n",
    "print(\"A total of {0} tweets are downloaded and saved to {1}\".format(tweet_count,fName))\n",
    "print(\"Total time taken is \",end-st,\"seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_query='Seyi Ma'\n",
    "x=0\n",
    "with open(fName2,'w') as f:\n",
    "    print(\"Downloading hashtag\" + search_query)\n",
    "    while(tweet_count<max_tweets):\n",
    "        try:\n",
    "            if(max_id<=0):\n",
    "                if(not since_id):\n",
    "                    new_tweets = api.search(q=search_query,count=tweets_per_query,lang=\"en\",tweet_mode='extended')\n",
    "\n",
    "                else:\n",
    "                    new_tweets = api.search(q=search_query,count=tweets_per_query,lang=\"en\",tweet_mode='extended',since_id=since_id)\n",
    "            else:\n",
    "                if(not since_id):\n",
    "                    new_tweets = api.search(q=search_query,count=tweets_per_query,lang=\"en\",tweet_mode='extended',max_id=str(max_id-1))\n",
    "                else:\n",
    "                    new_tweets = api.search(q=search_query,count=tweets_per_query,lang=\"en\",tweet_mode='extended',max_id=str(max_id-1),since_id=since_id)\n",
    "\n",
    "            # Tweets Exhausted\n",
    "            if(not new_tweets):\n",
    "                print(\"No more tweets found!!\")\n",
    "                break\n",
    "            # write all the new_tweets to a json file\n",
    "            for tweet in new_tweets:\n",
    "                f.write(jsonpickle.encode(tweet._json,unpicklable=False)+'\\n')\n",
    "                tweet_count+=len(new_tweets)\n",
    "                print(\"Successfully downloaded {0} tweets\".format(tweet_count))\n",
    "                max_id=new_tweets[-1].id\n",
    "        # in case of any error\n",
    "        except tweepy.TweepError as e:\n",
    "                print(\"Some error!!:\"+str(e))\n",
    "                break\n",
    "end = time.time()\n",
    "\n",
    "print(\"A total of {0} tweets are downloaded and saved to {1}\".format(tweet_count,fName))\n",
    "print(\"Total time taken is \",end-st,\"seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
